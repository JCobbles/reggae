{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Variational Inference\n",
    "\n",
    "The dataset required is small and is available preprocessed here:\n",
    "\n",
    "- https://drive.google.com/drive/folders/1Tg_3SlKbdv0pDog6k2ys0J79e1-vgRyd?usp=sharing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from gpytorch.optim import NGD\n",
    "from torch.optim import Adam\n",
    "from torch.nn import Parameter\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from alfi.datasets import ToyTranscriptomics, ToyTranscriptomicGenerator\n",
    "from alfi.configuration import VariationalConfiguration\n",
    "from alfi.models import TrainMode, generate_multioutput_gp\n",
    "from alfi.plot import Plotter1d, Colours, tight_kwargs\n",
    "from alfi.trainers import VariationalTrainer, PreEstimator\n",
    "from alfi.utilities.data import p53_ground_truth\n",
    "from alfi.impl.odes import TranscriptionLFM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's start by importing our dataset..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_genes = 11\n",
    "num_tfs = 3\n",
    "\n",
    "dataset = ToyTranscriptomicGenerator(\n",
    "    num_outputs=num_genes, num_latents=num_tfs, num_times=10, softplus=True, latent_data_present=True)\n",
    "dataset.generate_single()\n",
    "\n",
    "ground_truths = p53_ground_truth()\n",
    "class ConstrainedTrainer(VariationalTrainer):\n",
    "    def after_epoch(self):\n",
    "        with torch.no_grad():\n",
    "            sens = torch.tensor(1.)\n",
    "            dec = torch.tensor(0.8)\n",
    "            self.lfm.raw_sensitivity[3] = self.lfm.positivity.inverse_transform(sens)\n",
    "            self.lfm.raw_decay[3] = self.lfm.positivity.inverse_transform(dec)\n",
    "        super().after_epoch()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4, 2))\n",
    "for i in range(5):\n",
    "    plt.plot(dataset[i][1])\n",
    "plt.plot(dataset.f_observed[0, 0])\n",
    "t_end = dataset.t_observed[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use the ordinary differential equation (ODE):\n",
    "\n",
    "`dy/dt = b + sf(t) - dy`\n",
    "\n",
    "`f(t) ~ GP(0, k(t, t'))`\n",
    "\n",
    "Since this is an ODE, we inherit from the `OrdinaryLFM` class."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = VariationalConfiguration(\n",
    "    latent_data_present=True,\n",
    "    num_samples=75,\n",
    "    initial_conditions=False\n",
    ")\n",
    "\n",
    "num_inducing = 20  # (I x m x 1)\n",
    "inducing_points = torch.linspace(0, t_end, num_inducing).repeat(num_tfs, 1).view(num_tfs, num_inducing, 1)\n",
    "t_predict = torch.linspace(0, t_end+2, 80, dtype=torch.float32)\n",
    "step_size = 5e-1\n",
    "num_training = dataset.m_observed.shape[-1]\n",
    "use_natural = True\n",
    "gp_model = generate_multioutput_gp(num_tfs, inducing_points, gp_kwargs=dict(natural=use_natural))\n",
    "\n",
    "\n",
    "from alfi.utilities.torch import softplus\n",
    "class SoftplusTranscriptionLFM(TranscriptionLFM):\n",
    "    def __init__(self, num_outputs, gp_model, config: VariationalConfiguration, **kwargs):\n",
    "        super().__init__(num_outputs, gp_model, config, **kwargs)\n",
    "        num_latents = gp_model.variational_strategy.num_tasks\n",
    "        # self.sensitivity = Parameter(0.1 + torch.rand(torch.Size([self.num_outputs, 1]), dtype=torch.float32))\n",
    "        self.weight = Parameter(0.5 * torch.randn(torch.Size([self.num_outputs, num_latents]), dtype=torch.float32))\n",
    "        self.weight_bias = Parameter(torch.randn(torch.Size([self.num_outputs, 1]), dtype=torch.float32))\n",
    "\n",
    "    def initial_state(self):\n",
    "        return self.basal_rate / self.decay_rate\n",
    "\n",
    "    def nonlinearity(self, f):\n",
    "        return softplus(f)\n",
    "\n",
    "    def mix(self, f):\n",
    "        interactions = torch.matmul(self.weight, torch.log(f+1e-100)) + self.weight_bias\n",
    "        f = torch.sigmoid(interactions) # TF Activation Function (sigmoid)\n",
    "        return f\n",
    "\n",
    "lfm = SoftplusTranscriptionLFM(num_genes, gp_model, config, num_training_points=num_training)\n",
    "plotter = Plotter1d(lfm, dataset.gene_names, style='seaborn')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "track_parameters = [\n",
    "    'raw_basal',\n",
    "    'raw_decay',\n",
    "    'raw_sensitivity',\n",
    "    'gp_model.covar_module.raw_lengthscale',\n",
    "]\n",
    "if use_natural:\n",
    "    variational_optimizer = NGD(lfm.variational_parameters(), num_data=num_training, lr=0.09)\n",
    "    parameter_optimizer = Adam(lfm.nonvariational_parameters(), lr=0.02)\n",
    "    optimizers = [variational_optimizer, parameter_optimizer]\n",
    "    pre_variational_optimizer = NGD(lfm.variational_parameters(), num_data=num_training, lr=0.1)\n",
    "    pre_parameter_optimizer = Adam(lfm.nonvariational_parameters(), lr=0.005)\n",
    "    pre_optimizers = [pre_variational_optimizer, pre_parameter_optimizer]\n",
    "\n",
    "else:\n",
    "    optimizers = [Adam(lfm.parameters(), lr=0.05)]\n",
    "    pre_optimizers = [Adam(lfm.parameters(), lr=0.05)]\n",
    "\n",
    "trainer = ConstrainedTrainer(lfm, optimizers, dataset, track_parameters=track_parameters)\n",
    "pre_estimator = PreEstimator(lfm, pre_optimizers, dataset, track_parameters=track_parameters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Outputs prior to training:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "titles = ['Basal rates', 'Sensitivities', 'Decay rates']\n",
    "kinetics = list()\n",
    "for key in ['raw_basal', 'raw_sensitivity', 'raw_decay']:\n",
    "    kinetics.append(\n",
    "        lfm.positivity.transform(trainer.parameter_trace[key][-1].squeeze()).numpy())\n",
    "kinetics = np.array(kinetics)\n",
    "\n",
    "plotter.plot_double_bar(kinetics,\n",
    "                        # ground_truths=p53_ground_truth(),\n",
    "                        titles=titles)\n",
    "q_m = lfm.predict_m(t_predict, step_size=1e-1)\n",
    "q_f = lfm.predict_f(t_predict)\n",
    "print(q_m.mean.shape)\n",
    "plotter.plot_gp(q_m, t_predict, replicate=0,\n",
    "                t_scatter=dataset.t_observed,\n",
    "                y_scatter=dataset.m_observed, num_samples=0)\n",
    "plotter.plot_gp(q_f, t_predict, ylim=(-1, 3))\n",
    "plt.title('Latent')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lfm.set_mode(TrainMode.GRADIENT_MATCH)\n",
    "lfm.loss_fn.num_data = 61\n",
    "pre_estimator.train(50, report_interval=20);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lfm.set_mode(TrainMode.NORMAL)\n",
    "lfm.loss_fn.num_data = num_training\n",
    "trainer.train(100, report_interval=10, step_size=step_size);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lfm.eval()\n",
    "t_predict = torch.linspace(0, t_end+3, 80, dtype=torch.float32)\n",
    "\n",
    "# plotter.plot_losses(trainer, last_x=200)\n",
    "q_m = lfm.predict_m(t_predict, step_size=1e-1)\n",
    "q_f = lfm.predict_f(t_predict)\n",
    "\n",
    "\n",
    "titles = ['Basal rates', 'Sensitivities', 'Decay rates']\n",
    "kinetics = list()\n",
    "for key in ['raw_basal', 'raw_sensitivity', 'raw_decay']:\n",
    "    kinetics.append(\n",
    "        lfm.positivity.transform(trainer.parameter_trace[key][-1].squeeze()).numpy())\n",
    "kinetics = np.array(kinetics)\n",
    "plotter.plot_double_bar(kinetics,\n",
    "                        titles=titles,\n",
    "                        figsize=(6.5, 2.3),\n",
    "                        yticks=[\n",
    "                            np.linspace(0, 0.12, 5),\n",
    "                            np.linspace(0, 1.2, 4),\n",
    "                            np.arange(0, 1.1, 0.2),\n",
    "                        ])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./kinetics.pdf', **tight_kwargs)\n",
    "\n",
    "plotter.plot_gp(q_m, t_predict,\n",
    "                t_scatter=dataset.t_observed, y_scatter=dataset.m_observed)\n",
    "plotter.plot_gp(q_f, t_predict, t_scatter=dataset.t_observed, y_scatter=dataset.f_observed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-222640c7",
   "language": "python",
   "display_name": "PyCharm (reggae)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
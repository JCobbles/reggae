{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from lafomo.gp.variational.models import VariationalLFM\n",
    "from lafomo.gp.variational.trainer import P53ConstrainedTrainer\n",
    "from lafomo.gp.variational.options import VariationalOptions\n",
    "from lafomo.utilities import save, load\n",
    "from lafomo.plot.variational_plotters import Plotter\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from lafomo.data_loaders.datasets import TranscriptomicDataset\n",
    "from os import path\n",
    "import numpy as np\n",
    "class SingleCellKidney(TranscriptomicDataset):\n",
    "    \"\"\"\n",
    "    scRNA-seq dataset on the human kidney.\n",
    "    Accession number: GSE131685\n",
    "    Parameters:\n",
    "        calc_moments: bool=False whether to use the raw unspliced/spliced or moments\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir='../data/',\n",
    "                 raw_data_dir='/Volumes/ultra/genomics/scRNA-seq/GSE131685_RAW_kidney/velocyto',\n",
    "                 calc_moments=False):\n",
    "        super().__init__()\n",
    "        data_path = path.join(data_dir, 'kidney1.pt')\n",
    "        if path.exists(data_path):\n",
    "            data = torch.load(data_path)\n",
    "            self.m_observed = data['m_observed']\n",
    "            self.data = data['data']\n",
    "            self.gene_names = data['gene_names']\n",
    "        else:\n",
    "            import scvelo as scv\n",
    "            kidney1 = path.join(raw_data_dir, 'kidney1.loom')\n",
    "            data = scv.read_loom(kidney1)\n",
    "            scv.pp.filter_and_normalize(data, min_shared_counts=20, n_top_genes=2000)\n",
    "            if calc_moments:\n",
    "                scv.pp.moments(data, n_neighbors=30, n_pcs=30)\n",
    "                u = data.layers['Mu']\n",
    "                s = data.layers['Ms']\n",
    "            else:\n",
    "                u = data.layers['unspliced'].toarray()\n",
    "                s = data.layers['spliced'].toarray()\n",
    "\n",
    "            self.loom = data\n",
    "            self.gene_names = self.loom.var.index\n",
    "            self.data = np.concatenate([u, s], axis=1)\n",
    "            num_cells = self.data.shape[0]\n",
    "            self.data = torch.tensor(self.data.swapaxes(0, 1).reshape(4000, 1, num_cells))\n",
    "            self.m_observed = self.data.permute(1, 0, 2)\n",
    "\n",
    "            self.data = list(self.data)\n",
    "            torch.save({\n",
    "                'data': self.data,\n",
    "                'm_observed': self.m_observed,\n",
    "                'gene_names': self.gene_names\n",
    "            }, data_path)\n",
    "dataset = SingleCellKidney(calc_moments=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4000, 8164])\n",
      "(2000,)\n",
      "torch.Size([1, 8164])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.m_observed.shape)\n",
    "print(dataset.gene_names.shape)\n",
    "print(dataset[0].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from lafomo.data_loaders import LFMDataset\n",
    "from sortedcontainers import SortedList\n",
    "class RNAVelocityLFM(VariationalLFM):\n",
    "    def __init__(self, num_genes, num_latents, t_inducing, dataset: LFMDataset, options: VariationalOptions, **kwargs):\n",
    "        super().__init__(num_genes*2, num_latents, t_inducing, dataset, options, **kwargs)\n",
    "        self.transcription_rate = Parameter(torch.rand((num_genes, 1), dtype=torch.float64))\n",
    "        self.splicing_rate = Parameter(torch.rand((num_genes, 1), dtype=torch.float64))\n",
    "        self.decay_rate = Parameter(0.1 + torch.rand((num_genes, 1), dtype=torch.float64))\n",
    "        self.num_cells = dataset[0][0].shape[0]\n",
    "        ### Initialise random time assignments\n",
    "        self.time_assignments = Parameter(torch.rand(self.num_cells), requires_grad=False)\n",
    "\n",
    "    def odefunc(self, t, h):\n",
    "        \"\"\"h is of shape (num_samples, num_outputs, 1)\"\"\"\n",
    "        if (self.nfe % 5) == 0:\n",
    "            print(t)\n",
    "        self.nfe += 1\n",
    "        num_samples = h.shape[0]\n",
    "        num_outputs = h.shape[1]\n",
    "        h = h.view(num_samples, num_outputs//2, 2)\n",
    "        u = h[:, :, 0].unsqueeze(-1)\n",
    "        s = h[:, :, 1].unsqueeze(-1)\n",
    "        du = self.transcription_rate - self.splicing_rate * u\n",
    "        ds = self.splicing_rate * u - self.decay_rate * s\n",
    "\n",
    "        # q_f = self.get_latents(t.reshape(-1))\n",
    "        # # Reparameterisation trick\n",
    "        # f = q_f.rsample([self.num_samples])  # (S, I, t)\n",
    "        # f = self.G(f)  # (S, num_outputs, t)\n",
    "\n",
    "        h_t = torch.cat([du, ds], dim=1)\n",
    "        return h_t\n",
    "\n",
    "    def G(self, f):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            f: (I, T)\n",
    "        \"\"\"\n",
    "        return f\n",
    "\n",
    "    def predict_f(self, t_predict):\n",
    "        # Sample from the latent distribution\n",
    "        q_f = self.get_latents(t_predict.reshape(-1))\n",
    "        f = q_f.sample([500])  # (S, I, t)\n",
    "        # This is a hack to wrap the latent function with the nonlinearity. Note we use the same variance.\n",
    "        f = torch.mean(self.G(f), dim=0)[0]\n",
    "        return torch.distributions.multivariate_normal.MultivariateNormal(f, scale_tril=q_f.scale_tril)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from lafomo.utilities import is_cuda\n",
    "from lafomo.gp.variational.trainer import Trainer\n",
    "class EMTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Expectation-Maximisation Trainer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: .\n",
    "    optimizer:\n",
    "    dataset: Dataset where t_observed (T,), m_observed (J, T).\n",
    "    inducing timepoints.\n",
    "    give_output: whether the trainer should give the first output (y_0) as initial value to the model `forward()`\n",
    "    \"\"\"\n",
    "    def __init__(self, model: RNAVelocityLFM, optimizer: torch.optim.Optimizer, dataset, batch_size=1, give_output=False):\n",
    "        super().__init__(model, optimizer, dataset, batch_size, give_output)\n",
    "        # Initialise trajectory\n",
    "        self.timepoint_choices = torch.linspace(0, 1, 100, requires_grad=False)\n",
    "        initial_value = self.initial_value(None)\n",
    "        self.previous_trajectory = self.model(self.timepoint_choices, initial_value, rtol=1e-3, atol=1e-4)\n",
    "\n",
    "    def initial_value(self, y):\n",
    "        initial_value = torch.zeros((self.batch_size, 1), dtype=torch.float64)\n",
    "        initial_value = initial_value.cuda() if is_cuda() else initial_value\n",
    "        if self.give_output:\n",
    "            initial_value = y[0]\n",
    "        return initial_value.repeat(self.model.num_samples, 1, 1)  # Add batch dimension for sampling\n",
    "\n",
    "    def e_step(self, y):\n",
    "        num_outputs = self.model.num_outputs\n",
    "        # sorted_times, sort_indices = torch.sort(self.model.time_assignments, dim=0)\n",
    "        # trajectory = self.model(sorted_times, self.initial_value(None), rtol=1e-2, atol=1e-3)\n",
    "\n",
    "        # optimizer = torch.optim.LBFGS([model.time_assignments])\n",
    "        u = self.previous_trajectory[:num_outputs//2]  # (2000, 100, 1)\n",
    "        s = self.previous_trajectory[num_outputs//2:]  # (2000, 100, 1)\n",
    "        trajectory = self.previous_trajectory # (4000, 100, 1)\n",
    "        # y shape (4000, 8164, 1)\n",
    "        num_times = trajectory.shape[1]\n",
    "        index_to_time = torch.linspace(0, 1, num_times)\n",
    "        for cell in range(self.model.num_cells):\n",
    "            cell_time = self.model.time_assignments[cell]\n",
    "            minimum_residual = 1e9\n",
    "            minimum_i = -1\n",
    "            for i in range(num_times): # loop through all times\n",
    "                time = index_to_time[i]\n",
    "                residual = (trajectory[:, i] - y[:, cell]) ** 2\n",
    "                residual = residual.sum()\n",
    "                if residual < minimum_residual:\n",
    "                    minimum_residual = residual\n",
    "                    minimum_i = i\n",
    "            model.time_assignments[cell] = index_to_time[minimum_i]\n",
    "        #     def closure():\n",
    "        #         optimizer.zero_grad()\n",
    "        #         print(trajectory.shape, y.shape) # (4000, 100, 1)\n",
    "        #         # sum up the residuals between the true u, s and the trajectory u, s\n",
    "        #         # the model.time_assignments variable contains for each cell which time it is assigned\n",
    "        #         t_i = model.time_assignments[cell]\n",
    "        #         # trajectory[]\n",
    "        #\n",
    "        #         loss = (trajectory - y) ** 2\n",
    "        #         loss = loss.sum()\n",
    "        #         loss.backward()\n",
    "        #         print(loss)\n",
    "        #         return loss\n",
    "        #     optimizer.step(closure)\n",
    "\n",
    "    def single_epoch(self, rtol, atol):\n",
    "        epoch_loss = 0\n",
    "        epoch_ll = 0\n",
    "        epoch_kl = 0\n",
    "        for i, data in enumerate(self.data_loader):\n",
    "            self.optimizer.zero_grad()\n",
    "            y = data.permute(0, 2, 1) # (O, C, 1)\n",
    "            y = y.cuda() if is_cuda() else y\n",
    "            ### E-step ###\n",
    "            # assign timepoints $t_i$ to each cell by minimising its distance to the trajectory\n",
    "            # self.e_step(y)\n",
    "            print('estep done')\n",
    "            ### M-step ###\n",
    "            initial_value = self.initial_value(None)\n",
    "            t_sorted, inv_indices = torch.unique(self.model.time_assignments, sorted=True, return_inverse=True)\n",
    "            print(t_sorted, inv_indices)\n",
    "            print(t_sorted.shape)\n",
    "            output = self.model(t_sorted, initial_value, rtol=rtol, atol=atol)\n",
    "            print('fiin')\n",
    "            output = torch.squeeze(output)\n",
    "            print(output.shape)\n",
    "            # Calc loss and backprop gradients\n",
    "            mult = 1\n",
    "            if self.num_epochs <= 10:\n",
    "                mult = self.num_epochs/10\n",
    "\n",
    "            ll, kl = self.model.elbo(y, output, mult, data_index=i)\n",
    "            total_loss = -ll + kl\n",
    "            total_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            epoch_loss += total_loss.item()\n",
    "            epoch_ll += ll.item()\n",
    "            epoch_kl += kl.item()\n",
    "        return epoch_loss, epoch_ll, epoch_kl\n",
    "\n",
    "    def train(self, epochs=20, report_interval=1, plot_interval=20, rtol=1e-5, atol=1e-6):\n",
    "        losses = list()\n",
    "        end_epoch = self.num_epochs+epochs\n",
    "        plt.figure(figsize=(4, 2.3))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss, epoch_ll, epoch_kl = self.single_epoch(rtol, atol)\n",
    "\n",
    "\n",
    "            if (epoch % report_interval) == 0:\n",
    "                print('Epoch %d/%d - Loss: %.2f (%.2f %.2f) λ: %.3f' % (\n",
    "                    self.num_epochs + 1, end_epoch,\n",
    "                    epoch_loss, -epoch_ll, epoch_kl,\n",
    "                    self.model.kernel.lengthscale[0].item(),\n",
    "                ), end='')\n",
    "                self.print_extra()\n",
    "\n",
    "            losses.append((-epoch_ll, epoch_kl))\n",
    "            self.after_epoch()\n",
    "\n",
    "            if (epoch % plot_interval) == 0:\n",
    "                plt.plot(self.t_observed, output[0].cpu().detach().numpy(), label='epoch'+str(epoch))\n",
    "            self.num_epochs += 1\n",
    "        plt.legend()\n",
    "\n",
    "        losses = np.array(losses)\n",
    "        self.losses = np.concatenate([self.losses, losses], axis=0)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def print_extra(self):\n",
    "        print('')\n",
    "\n",
    "    def after_epoch(self):\n",
    "        pass\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8164\n",
      "tensor(0.)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(1.0000e-06, dtype=torch.float64)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(2.0000e-05, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(3.0000e-05, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(8.0000e-05, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(8.8889e-05, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(1.0000e-04, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(1.0000e-04, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0009, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0010, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0011, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0011, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0031, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0041, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0091, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0100, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0111, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0111, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0311, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0411, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0911, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.1000, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.1111, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.1111, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.3111, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.4111, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.9111, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(1.0000, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(1.1111, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(1.1111, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "outtt torch.Size([100, 50, 4000, 1])\n",
      "torch.Size([4000, 100, 1])  hout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacob/Documents/proj/reggae/lafomo/gp/variational/models/model.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.inducing_inputs = Parameter(torch.tensor(t_inducing), requires_grad=options.learn_inducing)\n",
      "/Users/jacob/Documents/proj/reggae/lafomo/gp/variational/models/model.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.initial_conditions = Parameter(torch.tensor(torch.zeros(self.num_outputs, 1)), requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "options = VariationalOptions(\n",
    "    learn_inducing=False,\n",
    "    num_samples=1,\n",
    "    kernel_scale=False\n",
    ")\n",
    "num_cells = dataset[0].shape[1]\n",
    "print(num_cells)\n",
    "t_inducing = torch.linspace(0, 1, 10, dtype=torch.float64).reshape((-1, 1))\n",
    "t_observed = torch.linspace(0, 12, num_cells).view(-1)\n",
    "t_predict = torch.linspace(-1, 13, 80, dtype=torch.float64)\n",
    "model = RNAVelocityLFM(2000, 1, t_inducing, dataset, options)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "trainer = EMTrainer(model, optimizer, dataset, batch_size=4000)\n",
    "plotter = Plotter(model, dataset.gene_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Outputs prior to training:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "rtol = 1e-3\n",
    "atol = rtol/10\n",
    "\n",
    "model_kwargs = {\n",
    "    'rtol': rtol, 'atol': atol\n",
    "}\n",
    "\n",
    "# plotter.plot_outputs(t_predict, replicate=0, t_scatter=t_observed,y_scatter=dataset.m_observed, model_kwargs=model_kwargs);\n",
    "# plotter.plot_latents(t_predict, ylim=(-1, 3), plot_barenco=True, plot_inducing=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estep done\n",
      "tensor([6.5565e-06, 6.5982e-05, 3.6025e-04,  ..., 9.9952e-01, 9.9959e-01,\n",
      "        9.9988e-01]) tensor([3772, 3341, 7108,  ..., 2982, 3011, 5788])\n",
      "torch.Size([8163])\n",
      "tensor(6.5565e-06)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(7.5565e-06, dtype=torch.float64)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(2.6557e-05, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(3.6557e-05, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(8.6557e-05, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(9.5445e-05, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0009, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0010, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0011, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0011, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0031, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0041, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0091, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0100, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0111, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0111, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0311, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0411, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.0911, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.1000, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.1111, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.1111, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.3111, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.4111, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(0.9111, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(1.0000, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(1.1111, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "tensor(1.1111, grad_fn=<AddBackward0>)\n",
      "torch.Size([50, 4000, 1])\n",
      "outtt torch.Size([8163, 50, 4000, 1])\n"
     ]
    }
   ],
   "source": [
    "tol = 5e-3\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "output = trainer.train(1, rtol=tol, atol=tol/10,\n",
    "                       report_interval=5, plot_interval=5)\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Outputs after training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plotter.plot_losses(trainer, last_x=100)\n",
    "plotter.plot_outputs(t_predict, replicate=0, ylim=(0, 3), t_scatter=t_observed,y_scatter=m_observed, model_kwargs=model_kwargs)\n",
    "plotter.plot_latents(t_predict, ylim=(-2, 4), plot_barenco=True, plot_inducing=False)\n",
    "plotter.plot_kinetics()\n",
    "plotter.plot_convergence(trainer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save(model, 'variational_linear')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "do_load = False\n",
    "if do_load:\n",
    "    model = load('variational_linear', SingleLinearLFM, num_genes, num_tfs,\n",
    "                 t_inducing, dataset, extra_points=2, fixed_variance=dataset.variance)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    trainer = P53ConstrainedTrainer(model, optimizer, dataset)\n",
    "print(do_load)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
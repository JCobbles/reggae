{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Variational Non-linear LFM\n",
    "\n",
    "In order to run this notebook yourself, you will need the dataset located here:\n",
    "- Go to https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE100099\n",
    "\n",
    "- Download the file `GSE100099_RNASeqGEO.tsv.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Parameter\n",
    "from gpytorch.distributions import MultitaskMultivariateNormal, MultivariateNormal\n",
    "\n",
    "from lafomo.models import OrdinaryLFM, MultiOutputGP\n",
    "from lafomo.trainer import TranscriptionalTrainer\n",
    "from lafomo.utilities.torch import save, load, softplus\n",
    "from lafomo.configuration import VariationalConfiguration\n",
    "from lafomo.datasets import HafnerData\n",
    "from lafomo.plot import Plotter\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = HafnerData(replicate=0, data_dir='../../../data/', extra_targets=False)\n",
    "num_replicates = 1\n",
    "num_genes = len(dataset.gene_names)\n",
    "num_tfs = 1\n",
    "num_times = dataset[0][0].shape[0]\n",
    "print(num_times)\n",
    "print(dataset[0][0], dataset.t)\n",
    "t_inducing = torch.linspace(0, 12, num_times, dtype=torch.float64)\n",
    "t_observed = torch.linspace(0, 12, num_times)\n",
    "t_predict = torch.linspace(-2, 14, 80, dtype=torch.float64)\n",
    "\n",
    "m_observed = torch.stack([\n",
    "    dataset[i][1] for i in range(num_genes*num_replicates)\n",
    "]).view(num_replicates, num_genes, num_times)\n",
    "\n",
    "plt.figure(figsize=(4, 2))\n",
    "for i in range(22):\n",
    "    plt.plot(dataset[i][1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TranscriptionLFM(OrdinaryLFM):\n",
    "    def __init__(self, num_outputs, gp_model, config: VariationalConfiguration):\n",
    "        super().__init__(num_outputs, gp_model, config)\n",
    "        self.decay_rate = Parameter(0.1 + torch.rand(torch.Size([self.num_outputs, 1]), dtype=torch.float64))\n",
    "        self.basal_rate = Parameter(torch.rand(torch.Size([self.num_outputs, 1]), dtype=torch.float64))\n",
    "        self.sensitivity = Parameter(0.2 + torch.rand(torch.Size([self.num_outputs, 1]), dtype=torch.float64))\n",
    "\n",
    "    def initial_state(self):\n",
    "        return self.basal_rate / self.decay_rate\n",
    "\n",
    "    def odefunc(self, t, h):\n",
    "        \"\"\"h is of shape (num_samples, num_outputs, 1)\"\"\"\n",
    "        self.nfe += 1\n",
    "        # if (self.nfe % 100) == 0:\n",
    "        #     print(t)\n",
    "\n",
    "        decay = self.decay_rate * h\n",
    "\n",
    "        f = self.f[:, :, self.t_index].unsqueeze(2)\n",
    "\n",
    "        h = self.basal_rate + self.sensitivity * f - decay\n",
    "        if t > self.last_t:\n",
    "            self.t_index += 1\n",
    "        self.last_t = t\n",
    "        return h\n",
    "\n",
    "    def G(self, f):\n",
    "        # I = 1 so just repeat for num_outputs\n",
    "        return softplus(f).repeat(1, self.num_outputs, 1)\n",
    "\n",
    "    def predict_f(self, t_predict):\n",
    "        # Sample from the latent distribution\n",
    "        q_f = super().predict_f(t_predict)\n",
    "        f = q_f.sample(torch.Size([500])).permute(0, 2, 1)  # (S, I, T)\n",
    "        print(f.shape)\n",
    "        # This is a hack to wrap the latent function with the nonlinearity. Note we use the same variance.\n",
    "        f = torch.mean(self.G(f), dim=0)[0].unsqueeze(0)\n",
    "        print(f.shape, q_f.mean.shape, q_f.scale_tril.shape)\n",
    "        batch_mvn = MultivariateNormal(f, q_f.covariance_matrix.unsqueeze(0))\n",
    "        print(batch_mvn)\n",
    "        return MultitaskMultivariateNormal.from_batch_mvn(batch_mvn, task_dim=0)\n",
    "\n",
    "class ExpTranscriptionLFM(TranscriptionLFM):\n",
    "\n",
    "    def G(self, f):\n",
    "        # I = 1 so just repeat for num_outputs\n",
    "        return torch.exp(f).repeat(1, self.num_outputs, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = VariationalConfiguration(\n",
    "    num_samples=70,\n",
    "    kernel_scale=False,\n",
    "    initial_conditions=False # TODO\n",
    ")\n",
    "\n",
    "num_inducing = 12  # (I x m x 1)\n",
    "inducing_points = torch.linspace(0, 12, num_inducing).repeat(num_tfs, 1).view(num_tfs, num_inducing, 1)\n",
    "t_predict = torch.linspace(-1, 13, 80, dtype=torch.float32)\n",
    "\n",
    "gp_model = MultiOutputGP(inducing_points, num_tfs)\n",
    "lfm = TranscriptionLFM(num_genes, gp_model, config)\n",
    "plotter = Plotter(lfm, np.array(dataset.gene_names))\n",
    "\n",
    "optimizer = torch.optim.Adam(lfm.parameters(), lr=0.05)\n",
    "trainer = TranscriptionalTrainer(lfm, optimizer, dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Outputs prior to training:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plotter.plot_outputs(t_predict,\n",
    "                     t_scatter=t_observed,\n",
    "                     y_scatter=m_observed,\n",
    "                     model_kwargs=dict(step_size=1e-1))\n",
    "plotter.plot_latents(t_predict,\n",
    "                     ylim=(-2, 5),\n",
    "                     plot_barenco=False,\n",
    "                     plot_inducing=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tol = 1e-1\n",
    "# trainer = Trainer(optimizer)\n",
    "output = trainer.train(200, step_size=1e-1, report_interval=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Outputs after training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plotter.plot_losses(trainer, last_x=100)\n",
    "plotter.plot_outputs(t_predict,\n",
    "                     replicate=0,\n",
    "                     t_scatter=t_observed,\n",
    "                     y_scatter=m_observed,\n",
    "                     model_kwargs=dict(step_size=1e-1))\n",
    "plotter.plot_latents(t_predict,\n",
    "                     ylim=(-2, 10),\n",
    "                     plot_inducing=False)\n",
    "\n",
    "# plotter.plot_convergence(trainer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(lfm.basal_rate)\n",
    "print(lfm.decay_rate)\n",
    "print(lfm.sensitivity)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "do_load = False\n",
    "if do_load:\n",
    "    model = load('nonlinear', NonLinearLFM, num_genes, num_tfs,\n",
    "                 t_inducing, dataset, extra_points=2, fixed_variance=dataset.variance)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    trainer = TranscriptionalTrainer(model, optimizer, dataset)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-222640c7",
   "language": "python",
   "display_name": "PyCharm (reggae)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
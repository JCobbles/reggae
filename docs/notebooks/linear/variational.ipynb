{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Variational Inference\n",
    "\n",
    "The dataset required is small and is available preprocessed here:\n",
    "\n",
    "- https://drive.google.com/drive/folders/1Tg_3SlKbdv0pDog6k2ys0J79e1-vgRyd?usp=sharing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import Parameter\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from os import path\n",
    "\n",
    "from lafomo.datasets import P53Data\n",
    "from lafomo.configuration import VariationalConfiguration\n",
    "from lafomo.models import OrdinaryLFM, MultiOutputGP\n",
    "from lafomo.plot import Plotter\n",
    "from lafomo.trainers import VariationalTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's start by importing our dataset..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = P53Data(replicate=0, data_dir='../../../data')\n",
    "num_genes = 5\n",
    "num_tfs = 1\n",
    "\n",
    "plt.figure(figsize=(4, 2))\n",
    "for i in range(5):\n",
    "    plt.plot(dataset[i][1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Differential equation here: ....\n",
    "\n",
    "Since this is an ordinary differential equation (ODE), we inherit from the `OrdinaryLFM` class."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TranscriptionLFM(OrdinaryLFM):\n",
    "    def __init__(self, num_outputs, gp_model, config: VariationalConfiguration):\n",
    "        super().__init__(num_outputs, gp_model, config)\n",
    "        self.decay_rate = Parameter(0.1 + torch.rand(torch.Size([self.num_outputs, 1]), dtype=torch.float64))\n",
    "        self.basal_rate = Parameter(torch.rand(torch.Size([self.num_outputs, 1]), dtype=torch.float64))\n",
    "        self.sensitivity = Parameter(0.2 + torch.rand(torch.Size([self.num_outputs, 1]), dtype=torch.float64))\n",
    "\n",
    "    def initial_state(self):\n",
    "        return self.basal_rate / self.decay_rate\n",
    "\n",
    "    def odefunc(self, t, h):\n",
    "        \"\"\"h is of shape (num_samples, num_outputs, 1)\"\"\"\n",
    "        self.nfe += 1\n",
    "        # if (self.nfe % 100) == 0:\n",
    "        #     print(t)\n",
    "\n",
    "        decay = self.decay_rate * h\n",
    "\n",
    "        f = self.f[:, :, self.t_index].unsqueeze(2)\n",
    "\n",
    "        h = self.basal_rate + self.sensitivity * f - decay\n",
    "        if t > self.last_t:\n",
    "            self.t_index += 1\n",
    "        self.last_t = t\n",
    "        return h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = VariationalConfiguration(\n",
    "    preprocessing_variance=dataset.variance,\n",
    "    num_samples=80,\n",
    "    kernel_scale=False,\n",
    "    initial_conditions=False\n",
    ")\n",
    "\n",
    "num_inducing = 12  # (I x m x 1)\n",
    "inducing_points = torch.linspace(0, 12, num_inducing).repeat(num_tfs, 1).view(num_tfs, num_inducing, 1)\n",
    "t_predict = torch.linspace(-1, 13, 80, dtype=torch.float32)\n",
    "step_size = 1e-1\n",
    "\n",
    "gp_model = MultiOutputGP(inducing_points, num_tfs)\n",
    "lfm = TranscriptionLFM(num_genes, gp_model, config)\n",
    "plotter = Plotter(lfm, dataset.gene_names, style='seaborn')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class P53ConstrainedTrainer(VariationalTrainer):\n",
    "    def after_epoch(self):\n",
    "        super().after_epoch()\n",
    "        # self.cholS.append(self.lfm.q_cholS.detach().clone())\n",
    "        # self.mus.append(self.lfm.q_m.detach().clone())\n",
    "        with torch.no_grad():\n",
    "            # TODO can we replace these with parameter transforms like we did with lengthscale\n",
    "            # self.lfm.sensitivity.clamp_(0, 20)\n",
    "            self.lfm.basal_rate.clamp_(0, 20)\n",
    "            self.lfm.decay_rate.clamp_(0, 20)\n",
    "            self.lfm.sensitivity[3] = np.float64(1.)\n",
    "            self.lfm.decay_rate[3] = np.float64(0.8)\n",
    "\n",
    "track_parameters = [\n",
    "    'basal_rate',\n",
    "    'decay_rate',\n",
    "    'sensitivity',\n",
    "    'gp_model.covar_module.raw_lengthscale',\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam(lfm.parameters(), lr=0.03)\n",
    "trainer = P53ConstrainedTrainer(lfm, optimizer, dataset, track_parameters=track_parameters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Outputs prior to training:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plotter.plot_kinetics()\n",
    "plotter.plot_outputs(t_predict, replicate=0,\n",
    "                     t_scatter=dataset.t_observed, y_scatter=dataset.m_observed,\n",
    "                     model_kwargs=dict(step_size=step_size))\n",
    "plotter.plot_latents(t_predict, ylim=(-1, 3), plot_barenco=False, plot_inducing=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lfm.train()\n",
    "\n",
    "trainer.train(10, report_interval=10, step_size=step_size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "root_dir = '../../../'\n",
    "experiment_dir = path.join(root_dir, 'experiments/variational')\n",
    "\n",
    "lfm.eval()\n",
    "plotter.plot_losses(trainer, last_x=200)\n",
    "plotter.plot_outputs(t_predict, replicate=0,\n",
    "                     t_scatter=dataset.t_observed, y_scatter=dataset.m_observed,\n",
    "                     model_kwargs=dict(step_size=step_size))\n",
    "plt.savefig(path.join(experiment_dir, 'outputs.pdf'), bbox_inches = 'tight', pad_inches = 0)\n",
    "\n",
    "plotter.plot_latents(t_predict, ylim=(-2, 3.2), plot_barenco=False, plot_inducing=False, num_samples=3)\n",
    "plt.savefig(path.join(experiment_dir, 'latents.pdf'), bbox_inches = 'tight', pad_inches = 0)\n",
    "\n",
    "plotter.plot_kinetics()\n",
    "plt.savefig(path.join(experiment_dir, 'kinetics.pdf'), bbox_inches = 'tight', pad_inches = 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-222640c7",
   "language": "python",
   "display_name": "PyCharm (reggae)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
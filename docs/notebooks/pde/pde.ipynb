{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from lafomo.datasets import ToySpatialTranscriptomics, P53Data\n",
    "from lafomo.kernels import SpatioTemporalRBF\n",
    "from lafomo.configuration import VariationalConfiguration\n",
    "from lafomo.utilities.torch import save, load\n",
    "from lafomo.plot import Plotter\n",
    "from lafomo.variational.models import ReactionDiffusion\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = P53Data(replicate=0)\n",
    "print(dataset[0][0].shape, dataset[0][0].dtype)\n",
    "dataset = ToySpatialTranscriptomics()\n",
    "print(dataset[0][0].shape, dataset[0][1].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('../data/demToy1GPGene.csv')\n",
    "data = pd.read_csv('../data/demToy1GPmRNA.csv')\n",
    "display(data.head(5))\n",
    "\n",
    "vals = data.values\n",
    "print(data.values[:, 1:3].shape)\n",
    "\n",
    "def plot_output(ax, output, title=None):\n",
    "    ax.set_title(title)\n",
    "    ax.plot(output)\n",
    "    ax.set_xlabel('distance')\n",
    "    ax.set_ylabel('y')\n",
    "def scatter_output(ax, output, title=None):\n",
    "    ax.set_title(title)\n",
    "    ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=output)\n",
    "    ax.set_xlabel('time')\n",
    "    ax.set_ylabel('distance')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "config = VariationalConfiguration(\n",
    "    initial_conditions=False,\n",
    "    num_samples=3\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.nn import Parameter\n",
    "from lafomo.datasets import LFMDataset\n",
    "from lafomo import LFM\n",
    "from lafomo.variational.models import VariationalLFM\n",
    "\n",
    "\n",
    "class PartialLFM(VariationalLFM):\n",
    "    def __init__(self, config: VariationalConfiguration, kernel, t_inducing,\n",
    "                 dataset: LFMDataset, dtype=torch.float64):\n",
    "        super().__init__(config, kernel, t_inducing, dataset, dtype)\n",
    "        if self.options.initial_conditions:\n",
    "            raise Exception('Initial conditions are not implemented for PartialLFM.')\n",
    "\n",
    "        T = 1.0            # final time\n",
    "        self.time_steps = 40\n",
    "        self.mesh_cells = 40\n",
    "        self.fenics_module = ReactionDiffusion(T/self.time_steps, self.mesh_cells)\n",
    "        self.sensitivity = Parameter(torch.ones((1, 1), dtype=torch.float64), requires_grad=True)\n",
    "        self.decay = Parameter(0.1*torch.ones((1, 1), dtype=torch.float64), requires_grad=True)\n",
    "        self.diffusion = Parameter(0.01*torch.ones((1, 1), dtype=torch.float64), requires_grad=True)\n",
    "\n",
    "    def forward(self, t, h, compute_var=False, return_samples=False):\n",
    "        \"\"\"\n",
    "        t : torch.Tensor\n",
    "            Shape (num_times)\n",
    "        h : torch.Tensor the initial state of the ODE\n",
    "            Shape (num_genes, 1)\n",
    "        Returns\n",
    "        -------\n",
    "        Returns evolved h across times t.\n",
    "        Shape (num_genes, num_points).\n",
    "        \"\"\"\n",
    "        self.nfe = 0\n",
    "\n",
    "        # Precompute variables\n",
    "        self.Kmm = self.kernel(self.inducing_inputs)\n",
    "        self.L = torch.cholesky(self.Kmm)\n",
    "        q_cholS = torch.tril(self.q_cholS)\n",
    "        self.S = torch.matmul(q_cholS, torch.transpose(q_cholS, 1, 2))\n",
    "\n",
    "        # Integrate forward from the initial positions h0.\n",
    "\n",
    "        ####\n",
    "        outputs = list()\n",
    "        y_prev = torch.zeros((self.options.num_samples, self.mesh_cells + 1), requires_grad=False, dtype=torch.float64)\n",
    "        t_index = 0\n",
    "        tx = torch.tensor(data.values[:, 0:2]).permute(1, 0)\n",
    "        q_u = self.get_latents(tx)\n",
    "        print('mean', q_u.mean.shape)\n",
    "\n",
    "        for n in range(self.time_steps + 1):\n",
    "            u = data[data['t'] == t[t_index]]['U'].values\n",
    "            u = torch.tensor(u, requires_grad=False).unsqueeze(0)\n",
    "\n",
    "            q_u = self.get_latents(tx[:, t_index::41])\n",
    "            # Reparameterisation trick\n",
    "            u = q_u.rsample([self.options.num_samples])  # (S, I, t)\n",
    "            u = self.G(u)  # (S, num_outputs, t)\n",
    "            # TODO use samples\n",
    "            u = u[:,0,:]\n",
    "            # print(u.shape)\n",
    "            # plt.plot(u[0].detach())\n",
    "            # print(u.shape)\n",
    "\n",
    "            sensitivity = self.sensitivity.repeat(self.options.num_samples, 1)\n",
    "            decay = self.decay.repeat(self.options.num_samples, 1)\n",
    "            diffusion = self.diffusion.repeat(self.options.num_samples, 1)\n",
    "\n",
    "            y_prev = self.fenics_module(y_prev, u,\n",
    "                                        sensitivity, decay, diffusion)\n",
    "\n",
    "            # y_prev shape (N, 21)\n",
    "            t_index += 1\n",
    "            outputs.append(y_prev)\n",
    "        outputs = torch.stack(outputs).permute(1, 0, 2).mean(dim=0).unsqueeze(0)  # shape (batch, times, distance)\n",
    "        print(outputs.shape)\n",
    "        return outputs\n",
    "        ####\n",
    "        # h_samples = odeint(self.odefunc, h0, t, )  # (T, S, num_outputs, 1)\n",
    "        #\n",
    "        # if return_samples:\n",
    "        #     return h_samples\n",
    "        #\n",
    "        # h_out = torch.mean(h_samples, dim=1).transpose(0, 1)\n",
    "        # h_std = torch.std(h_samples, dim=1).transpose(0, 1)\n",
    "        #\n",
    "        # if compute_var:\n",
    "        #     return self.decode(h_out), h_std\n",
    "        # return self.decode(h_out)\n",
    "\n",
    "    def G(self, u):\n",
    "        return u"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from lafomo.variational.trainer import Trainer\n",
    "\n",
    "class PDETrainer(Trainer):\n",
    "\n",
    "    def single_epoch(self, *args):\n",
    "        epoch_loss = 0\n",
    "        epoch_ll = 0\n",
    "        epoch_kl = 0\n",
    "        output = None\n",
    "        # for i, data in enumerate(self.data_loader):\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        # t, y = data\n",
    "        # t = t.cuda() if is_cuda() else t\n",
    "        # y = y.cuda() if is_cuda() else y\n",
    "        # # Assume that the batch of t s are the same\n",
    "        # t, y = t[0].view(-1), y\n",
    "        # initial_value = self.initial_value(y)\n",
    "        initial_value = None\n",
    "        t = data['t'].values\n",
    "\n",
    "        output = self.model(t, initial_value)\n",
    "\n",
    "        y_target = torch.tensor(data['Y'].values).unsqueeze(0)\n",
    "        print('y_target shape', y_target.shape)\n",
    "        print('output shape', output.shape)\n",
    "        output = output.transpose(1, 2).reshape(-1)\n",
    "        # print(output.shape)\n",
    "        # loss = self.loss_fn(y_prev, y_target)\n",
    "        # loss.backward()\n",
    "        fig, axes = plt.subplots(ncols=2)\n",
    "        scatter_output(axes[0], output.detach(), 'Prediction')\n",
    "        scatter_output(axes[1], data.iloc[:, 3], 'Actual')\n",
    "\n",
    "        # Calc loss and backprop gradients\n",
    "        mult = 1\n",
    "        if self.num_epochs <= 10:\n",
    "            mult = self.num_epochs/10\n",
    "\n",
    "        ll, kl = self.model.elbo(y_target, output, mult)\n",
    "        total_loss = -ll + kl\n",
    "\n",
    "        total_loss.backward()\n",
    "        # print('ses', self.model.sensitivity)\n",
    "        print('grad', self.model.kernel.raw_lengthscale.grad)\n",
    "\n",
    "        self.optimizer.step()\n",
    "        # print(\"ses\", self.model.sensitivity)\n",
    "\n",
    "        epoch_loss += total_loss.item()\n",
    "        epoch_ll += ll.item()\n",
    "        epoch_kl += kl.item()\n",
    "\n",
    "        # end for\n",
    "        return output, epoch_loss, (-epoch_ll, epoch_kl)\n",
    "\n",
    "    def print_extra(self):\n",
    "        print(' s:', model.sensitivity[0].item(),\n",
    "              'dif:', model.diffusion[0].item(),\n",
    "              'dec:', model.decay[0].item())\n",
    "\n",
    "\n",
    "t_inducing = torch.linspace(0, 1, 10, dtype=torch.float64).view(1, -1).repeat(2, 1)\n",
    "print(t_inducing.shape)\n",
    "\n",
    "dataset = ToySpatialTranscriptomics()\n",
    "kernel = SpatioTemporalRBF(dataset.num_latents, initial_lengthscale=0.3)\n",
    "\n",
    "model = PartialLFM(config, kernel, t_inducing, dataset)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "trainer = PDETrainer(model, optimizer, dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.train(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(model.kernel.lengthscale, model.sensitivity, model.diffusion)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2)\n",
    "scatter_output(axes[0], us.transpose(1, 2).reshape(-1), 'Prediction')\n",
    "scatter_output(axes[1], data.iloc[:, 3], 'Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig, axes = plt.subplots(ncols=2)\n",
    "plot_output(axes[0], us[0, 4, :], 'Prediction')\n",
    "plot_output(axes[1], data[data['t'] == ts[4]].iloc[:, 3], 'Actual')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(data[data['t'] == ts[5]].iloc[:, 2])\n",
    "plt.xlabel('distance')\n",
    "plt.ylabel('u')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
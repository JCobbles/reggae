{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Variational Inference\n",
    "\n",
    "The dataset required is small and is available preprocessed here:\n",
    "\n",
    "- https://drive.google.com/drive/folders/1Tg_3SlKbdv0pDog6k2ys0J79e1-vgRyd?usp=sharing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from gpytorch.optim import NGD\n",
    "from torch.optim import Adam\n",
    "from torch.nn import Parameter\n",
    "from matplotlib import pyplot as plt\n",
    "from os import path\n",
    "\n",
    "from lafomo.datasets import P53Data, ToyTranscriptomicGenerator\n",
    "from lafomo.configuration import VariationalConfiguration\n",
    "from lafomo.models import OrdinaryLFM, MultiOutputGP, generate_multioutput_rbf_gp\n",
    "from lafomo.plot import Plotter1d, Colours, tight_kwargs\n",
    "from lafomo.trainers import VariationalTrainer, PreEstimator\n",
    "from lafomo.utilities.data import p53_ground_truth\n",
    "from experiments.mae import get_datasets\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's start by importing our dataset..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p53 = True\n",
    "if p53:\n",
    "    dataset = P53Data(replicate=0, data_dir='../../../data')\n",
    "    ground_truths = p53_ground_truth()\n",
    "    class ConstrainedTrainer(VariationalTrainer):\n",
    "        def after_epoch(self):\n",
    "            with torch.no_grad():\n",
    "                sens = torch.tensor(1.)\n",
    "                dec = torch.tensor(0.8)\n",
    "                self.lfm.raw_sensitivity[3] = self.lfm.positivity.inverse_transform(sens)\n",
    "                self.lfm.raw_decay[3] = self.lfm.positivity.inverse_transform(dec)\n",
    "            super().after_epoch()\n",
    "\n",
    "else:\n",
    "    datasets = get_datasets(data_dir='../../../experiments')\n",
    "    dataset = datasets[5]\n",
    "    # dataset.generate_single(lengthscale=1.3)\n",
    "    dataset.variance = 1e-5 * torch.ones(dataset.m_observed.shape[-1], dtype=torch.float32)\n",
    "    ground_truths = [\n",
    "        dataset.lfm.basal_rate.detach().view(-1).numpy(),\n",
    "        dataset.lfm.sensitivity.detach().view(-1).numpy(),\n",
    "        dataset.lfm.decay_rate.detach().view(-1).numpy()\n",
    "    ]\n",
    "    class ConstrainedTrainer(VariationalTrainer):\n",
    "        def after_epoch(self):\n",
    "            with torch.no_grad():\n",
    "                sens = dataset.lfm.sensitivity[0].squeeze()\n",
    "                dec = dataset.lfm.decay_rate[0].squeeze()\n",
    "                self.lfm.raw_sensitivity[0] = self.lfm.positivity.inverse_transform(sens)\n",
    "                self.lfm.raw_decay[0] = self.lfm.positivity.inverse_transform(dec)\n",
    "            super().after_epoch()\n",
    "\n",
    "\n",
    "num_genes = 5\n",
    "num_tfs = 1\n",
    "\n",
    "plt.figure(figsize=(4, 2))\n",
    "for i in range(5):\n",
    "    plt.plot(dataset[i][1])\n",
    "plt.plot(dataset.f_observed[0, 0])\n",
    "t_end = dataset.t_observed[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use the ordinary differential equation (ODE):\n",
    "\n",
    "`dy/dt = b + sf(t) - dy`\n",
    "\n",
    "`f(t) ~ GP(0, k(t, t'))`\n",
    "\n",
    "Since this is an ODE, we inherit from the `OrdinaryLFM` class."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gpytorch.constraints import Positive\n",
    "class TranscriptionLFM(OrdinaryLFM):\n",
    "    def __init__(self, num_outputs, gp_model, config: VariationalConfiguration, **kwargs):\n",
    "        super().__init__(num_outputs, gp_model, config, **kwargs)\n",
    "        self.positivity = Positive()\n",
    "        self.raw_decay = Parameter(\n",
    "            self.positivity.inverse_transform(0.1 + torch.rand(torch.Size([self.num_outputs, 1]), dtype=torch.float64)))\n",
    "        self.raw_basal = Parameter(\n",
    "            self.positivity.inverse_transform(0.1 * torch.rand(torch.Size([self.num_outputs, 1]), dtype=torch.float64)))\n",
    "        self.raw_sensitivity = Parameter(\n",
    "            self.positivity.inverse_transform(2*torch.rand(torch.Size([self.num_outputs, 1]), dtype=torch.float64)))\n",
    "\n",
    "    @property\n",
    "    def decay_rate(self):\n",
    "        return self.positivity.transform(self.raw_decay)\n",
    "\n",
    "    @decay_rate.setter\n",
    "    def decay_rate(self, value):\n",
    "        self.raw_decay = self.positivity.inverse_transform(value)\n",
    "\n",
    "    @property\n",
    "    def basal_rate(self):\n",
    "        return self.positivity.transform(self.raw_basal)\n",
    "\n",
    "    @basal_rate.setter\n",
    "    def basal_rate(self, value):\n",
    "        self.raw_basal = self.positivity.inverse_transform(value)\n",
    "\n",
    "    @property\n",
    "    def sensitivity(self):\n",
    "        return self.positivity.transform(self.raw_sensitivity)\n",
    "\n",
    "    @sensitivity.setter\n",
    "    def sensitivity(self, value):\n",
    "        self.raw_sensitivity = self.decay_constraint.inverse_transform(value)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return self.basal_rate / self.decay_rate\n",
    "\n",
    "    def odefunc(self, t, h):\n",
    "        \"\"\"h is of shape (num_samples, num_outputs, 1)\"\"\"\n",
    "        self.nfe += 1\n",
    "        # if (self.nfe % 100) == 0:\n",
    "        #     print(t)\n",
    "        f = self.f\n",
    "        if not self.pretrain_mode:\n",
    "            f = self.f[:, :, self.t_index].unsqueeze(2)\n",
    "            if t > self.last_t:\n",
    "                self.t_index += 1\n",
    "            self.last_t = t\n",
    "\n",
    "        dh = self.basal_rate + self.sensitivity * f - self.decay_rate * h\n",
    "        return dh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = VariationalConfiguration(\n",
    "    preprocessing_variance=dataset.variance,\n",
    "    num_samples=80,\n",
    "    initial_conditions=False\n",
    ")\n",
    "\n",
    "num_inducing = 20  # (I x m x 1)\n",
    "inducing_points = torch.linspace(0, t_end, num_inducing).repeat(num_tfs, 1).view(num_tfs, num_inducing, 1)\n",
    "t_predict = torch.linspace(0, t_end, 80, dtype=torch.float32)\n",
    "# t_predict = torch.linspace(0, t_end+2, 80, dtype=torch.float32)\n",
    "step_size = 5e-1\n",
    "num_training = dataset.m_observed.shape[-1]\n",
    "use_natural = True\n",
    "gp_model = generate_multioutput_rbf_gp(num_tfs, inducing_points, gp_kwargs=dict(natural=use_natural))\n",
    "\n",
    "lfm = TranscriptionLFM(num_genes, gp_model, config, num_training_points=num_training)\n",
    "plotter = Plotter1d(lfm, dataset.gene_names, style='seaborn')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "track_parameters = [\n",
    "    'raw_basal',\n",
    "    'raw_decay',\n",
    "    'raw_sensitivity',\n",
    "    'gp_model.covar_module.raw_lengthscale',\n",
    "]\n",
    "if use_natural:\n",
    "    variational_optimizer = NGD(lfm.variational_parameters(), num_data=num_training, lr=0.09)\n",
    "    parameter_optimizer = Adam(lfm.nonvariational_parameters(), lr=0.02)\n",
    "    optimizers = [variational_optimizer, parameter_optimizer]\n",
    "    pre_variational_optimizer = NGD(lfm.variational_parameters(), num_data=num_training, lr=0.1)\n",
    "    pre_parameter_optimizer = Adam(lfm.nonvariational_parameters(), lr=0.005)\n",
    "    pre_optimizers = [pre_variational_optimizer, pre_parameter_optimizer]\n",
    "\n",
    "else:\n",
    "    optimizers = [Adam(lfm.parameters(), lr=0.05)]\n",
    "    pre_optimizers = [Adam(lfm.parameters(), lr=0.05)]\n",
    "\n",
    "trainer = ConstrainedTrainer(lfm, optimizers, dataset, track_parameters=track_parameters)\n",
    "pre_estimator = PreEstimator(lfm, pre_optimizers, dataset, track_parameters=track_parameters)\n",
    "strat = lfm.gp_model.variational_strategy.base_variational_strategy\n",
    "dist = strat._variational_distribution\n",
    "# plt.imshow(dist.chol_variational_covar.detach().squeeze())\n",
    "# plt.colorbar()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Outputs prior to training:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "titles = ['Basal rates', 'Sensitivities', 'Decay rates']\n",
    "kinetics = list()\n",
    "for key in ['raw_basal', 'raw_sensitivity', 'raw_decay']:\n",
    "    kinetics.append(\n",
    "        lfm.positivity.transform(trainer.parameter_trace[key][-1].squeeze()).numpy())\n",
    "kinetics = np.array(kinetics)\n",
    "print(kinetics.shape)\n",
    "plotter.plot_double_bar(kinetics,\n",
    "                        ground_truths=p53_ground_truth(),\n",
    "                        titles=titles)\n",
    "q_m = lfm.predict_m(t_predict, step_size=1e-1)\n",
    "q_f = lfm.predict_f(t_predict)\n",
    "\n",
    "plotter.plot_gp(q_m, t_predict, replicate=0,\n",
    "                t_scatter=dataset.t_observed,\n",
    "                y_scatter=dataset.m_observed, num_samples=0)\n",
    "plotter.plot_gp(q_f, t_predict, ylim=(-1, 3))\n",
    "plt.title('Latent')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lfm.pretrain(True)\n",
    "# lfm.loss_fn.num_data = 61\n",
    "# pre_estimator.train(50, report_interval=20);\n",
    "\n",
    "print(num_training)\n",
    "\n",
    "from torch.nn.functional import l1_loss\n",
    "m_targ = dataset.m_observed_highres.squeeze().t()\n",
    "f_targ = dataset.f_observed_highres.squeeze(0).t()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lfm.pretrain(False)\n",
    "t_predict = torch.linspace(0, t_end, 111)\n",
    "f_maes = list()\n",
    "m_maes = list()\n",
    "for i in range(700 // 10):\n",
    "    trainer.train(epochs=10, report_interval=50, step_size=5e-1)\n",
    "    m_pred = lfm.predict_m(t_predict, jitter=1e-3)\n",
    "    f_pred = lfm.predict_f(t_predict, jitter=1e-3)\n",
    "    m_mae = l1_loss(m_pred.mean, m_targ).mean().item()\n",
    "    f_mae = l1_loss(f_pred.mean, f_targ).mean().item()\n",
    "    f_maes.append(f_mae)\n",
    "    m_maes.append(m_mae)\n",
    "\n",
    "\n",
    "f_maes = torch.tensor(f_maes)\n",
    "m_maes = torch.tensor(m_maes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "min_index = (f_maes + m_maes).argmin(dim=0)\n",
    "f_mae = f_maes[min_index]\n",
    "m_mae = m_maes[min_index]\n",
    "\n",
    "print(t_end)\n",
    "print(m_maes)\n",
    "print(f_maes)\n",
    "print(m_mae, f_mae)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lfm.pretrain(False)\n",
    "lfm.loss_fn.num_data = num_training\n",
    "step_size = 5e-1\n",
    "trainer.train(50, report_interval=10, step_size=step_size);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t_predict = torch.linspace(0, t_end, 111, dtype=torch.float32)\n",
    "\n",
    "# plotter.plot_losses(trainer, last_x=200)\n",
    "q_m = lfm.predict_m(t_predict, step_size=1e-1)\n",
    "q_f = lfm.predict_f(t_predict)\n",
    "\n",
    "\n",
    "labels = ['Basal rates', 'Sensitivities', 'Decay rates']\n",
    "kinetics = list()\n",
    "for key in ['raw_basal', 'raw_sensitivity', 'raw_decay']:\n",
    "    kinetics.append(\n",
    "        lfm.positivity.transform(trainer.parameter_trace[key][-1].squeeze()).numpy())\n",
    "\n",
    "plotter.plot_double_bar(kinetics, labels, ground_truths=ground_truths,\n",
    "                        figsize=(6.5, 2.3),\n",
    "                        yticks=[\n",
    "                            np.linspace(0, 0.12, 5),\n",
    "                            np.linspace(0, 1.2, 4),\n",
    "                            np.arange(0, 1.1, 0.2),\n",
    "                        ])\n",
    "plt.tight_layout()\n",
    "plt.savefig('./kinetics.pdf', **tight_kwargs)\n",
    "\n",
    "plotter.plot_gp(q_m, t_predict,\n",
    "                t_scatter=dataset.t_observed, y_scatter=dataset.m_observed)\n",
    "plotter.plot_gp(q_f, t_predict, t_scatter=dataset.t_observed, y_scatter=dataset.f_observed)\n",
    "plt.plot(t_predict, f_targ.squeeze())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 1.6),\n",
    "                         gridspec_kw=dict(width_ratios=[1, 1, 0.3, 1.9], wspace=0))\n",
    "\n",
    "row = 0\n",
    "col = 0\n",
    "ub = [3.5, 3.5]\n",
    "for i in range(2):\n",
    "    ax = axes[i]\n",
    "    plotter.plot_gp(q_m, t_predict, replicate=0, ax=ax,\n",
    "                    color=Colours.line_color, shade_color=Colours.shade_color,\n",
    "                    t_scatter=dataset.t_observed, y_scatter=dataset.m_observed,\n",
    "                    num_samples=0, only_plot_index=i)\n",
    "    ax.set_ylim([-0.2, ub[i]])\n",
    "    ax.set_yticks([0, 3])\n",
    "    ax.set_title(dataset.gene_names[i])\n",
    "    ax.set_xlim(-0.4, 15)\n",
    "    if col > 0:\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticks([5, 10, 15])\n",
    "\n",
    "    col += 1\n",
    "plotter.plot_gp(q_f, t_predict, ax=axes[3],\n",
    "                ylim=(-1, 3.2),\n",
    "                num_samples=3,\n",
    "                color=Colours.line2_color,\n",
    "                shade_color=Colours.shade2_color)\n",
    "plotter.plot_gp(exact_q_f, t_predict, ax=axes[3],\n",
    "                ylim=(-1, 3.2), color=Colours.line_color,\n",
    "                shade_color='red')\n",
    "axes[3].set_title('Latent force (p53)')\n",
    "axes[3].set_yticks([-1, 3])\n",
    "axes[3].set_xlabel('Time (h)')\n",
    "axes[3].set_xlim(0, 15)\n",
    "axes[3].set_xticks([0, 5, 10, 15])\n",
    "axes[2].set_visible(False)\n",
    "\n",
    "# plt.savefig('./barenco-combined.pdf', **tight_kwargs)\n",
    "B_exact, S_exact, D_exact = p53_ground_truth()\n",
    "B_exact, S_exact, D_exact = np.array(B_exact), np.array(S_exact), np.array(D_exact)\n",
    "B = lfm.basal_rate.detach().squeeze()\n",
    "D = lfm.decay_rate.detach().squeeze()\n",
    "S = lfm.basal_rate.detach().squeeze()\n",
    "mse = torch.square(B-B_exact) + torch.square(D-D_exact) + torch.square(S-S_exact)\n",
    "print(D)\n",
    "print(mse.mean(), (D-D_exact), B_exact.shape)\n",
    "mae2 = l1_loss(q_m.mean, exact_q_m.mean)\n",
    "print(mae2, q_m.mean.shape, exact_q_m.mean.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
    "\n",
    "from lafomo.models import ExactLFM\n",
    "from lafomo.trainers import ExactTrainer\n",
    "\n",
    "exact_lfm = ExactLFM(dataset, dataset.variance.reshape(-1))\n",
    "optimizer = torch.optim.Adam(exact_lfm.parameters(), lr=0.07)\n",
    "\n",
    "loss_fn = ExactMarginalLogLikelihood(exact_lfm.likelihood, exact_lfm)\n",
    "\n",
    "track_parameters = [\n",
    "    'mean_module.raw_basal',\n",
    "    'covar_module.raw_decay',\n",
    "    'covar_module.raw_sensitivity',\n",
    "    'covar_module.raw_lengthscale',\n",
    "]\n",
    "exact_trainer = ExactTrainer(exact_lfm, [optimizer], dataset, loss_fn=loss_fn, track_parameters=track_parameters)\n",
    "plotter = Plotter(exact_lfm, dataset.gene_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exact_lfm.train()\n",
    "exact_lfm.likelihood.train()\n",
    "exact_trainer.train(epochs=150, report_interval=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exact_q_m = exact_lfm.predict_m(t_predict)\n",
    "exact_q_f = exact_lfm.predict_f(t_predict)\n",
    "print(exact_q_f)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " # key in ['basal_rate', 'sensitivity', 'decay_rate']:\n",
    "plt.plot(lfm.positivity.transform(torch.stack(trainer.parameter_trace['raw_basal'])[:, 3]))\n",
    "plt.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(lfm.basal_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## This stuff is safe to delete:\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(9, 2.7),\n",
    "                         gridspec_kw=dict(width_ratios=[1, 1, 0.3, 1.8], wspace=0, hspace=0.8))\n",
    "\n",
    "row = 0\n",
    "col = 0\n",
    "ub = [3.5] * 4\n",
    "for i in range(4):\n",
    "    if i == 2:\n",
    "        row += 1\n",
    "        col = 0\n",
    "    ax = axes[row, col]\n",
    "    plotter.plot_gp(q_m, t_predict, replicate=0, ax=ax,\n",
    "                    color=Colours.line_color, shade_color=Colours.shade_color,\n",
    "                    t_scatter=dataset.t_observed, y_scatter=dataset.m_observed,\n",
    "                    num_samples=0, only_plot_index=i)\n",
    "    ax.set_ylim([-0.2, ub[i]])\n",
    "    ax.set_yticks([0, 3])\n",
    "    ax.set_title(dataset.gene_names[i])\n",
    "    ax.set_xlim(-0.4, 15)\n",
    "    if col > 0:\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticks([5, 10, 15])\n",
    "\n",
    "    col += 1\n",
    "plotter.plot_gp(q_f, t_predict, ax=axes[1, 3],\n",
    "                ylim=(-1, 3.2),\n",
    "                num_samples=0,\n",
    "                color=Colours.line2_color,\n",
    "                shade_color=Colours.shade2_color)\n",
    "plotter.plot_gp(exact_q_f, t_predict, ax=axes[0, 3],\n",
    "                ylim=(-1, 3.2), color=Colours.line2_color,\n",
    "                shade_color=Colours.shade2_color)\n",
    "titles = ['Lawrence et al., 2007', 'ours']\n",
    "for i in range(2):\n",
    "    axes[i, 3].set_title(f'Latent force ({titles[i]})')\n",
    "    axes[i, 3].set_yticks([-1, 3])\n",
    "    axes[i, 3].set_xlim(0, 15)\n",
    "    axes[i, 3].set_xticks([0, 5, 10, 15])\n",
    "    axes[i, 2].set_visible(False)\n",
    "axes[1, 3].set_xlabel('Time (h)')\n",
    "\n",
    "plt.savefig('./barenco-combined.pdf', **tight_kwargs)\n",
    "\n",
    "from torch.nn.functional import l1_loss\n",
    "print(l1_loss(q_f.mean, exact_q_f.mean).item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-222640c7",
   "language": "python",
   "display_name": "PyCharm (reggae)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
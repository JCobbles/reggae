{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from abc import abstractmethod\n",
    "import torch\n",
    "import gpytorch\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from gpytorch.likelihoods import MultitaskGaussianLikelihood\n",
    "from torchdiffeq import odeint\n",
    "from torch.nn import Parameter\n",
    "from torch.distributions import Independent, Normal\n",
    "\n",
    "from lafomo.datasets import P53Data\n",
    "from lafomo.configuration import VariationalConfiguration\n",
    "from lafomo.variational.models import VariationalLFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class MultitaskGPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points, num_latents):\n",
    "        # We have to mark the CholeskyVariationalDistribution as batch\n",
    "        # so that we learn a variational distribution for each task\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            inducing_points.size(-2), batch_shape=torch.Size([num_latents])\n",
    "        )\n",
    "\n",
    "        # We have to wrap the VariationalStrategy in a MultitaskVariationalStrategy\n",
    "        # so that the output will be a MultitaskMultivariateNormal rather than a batch output\n",
    "        variational_strategy = gpytorch.variational.IndependentMultitaskVariationalStrategy(\n",
    "            gpytorch.variational.VariationalStrategy(\n",
    "                self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "            ), num_tasks=num_latents\n",
    "        )\n",
    "\n",
    "        super().__init__(variational_strategy)\n",
    "\n",
    "        # The mean and covariance modules should be marked as batch\n",
    "        # so we learn a different set of hyperparameters\n",
    "        self.mean_module = gpytorch.means.ZeroMean(batch_shape=torch.Size([num_latents]))\n",
    "        self.covar_module = gpytorch.kernels.RBFKernel(batch_shape=torch.Size([num_latents]))\n",
    "\n",
    "    def forward(self, t):\n",
    "        # The forward function should be written as if we were dealing with each output\n",
    "        # dimension in batch\n",
    "        mean_x = self.mean_module(t)\n",
    "        covar_x = self.covar_module(t)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "\n",
    "class OrdinaryLFM(VariationalLFM):\n",
    "    \"\"\"\n",
    "    Variational approximation for an LFM based on an ordinary differential equation (ODE).\n",
    "    Inheriting classes must override the `odefunc` function which encodes the ODE.\n",
    "    \"\"\"\n",
    "\n",
    "    # def __init__(self, num_latent, config: VariationalConfiguration, kernel, t_inducing, dataset: LFMDataset, dtype=torch.float64):\n",
    "        # super().__init__(num_latent, config, kernel, t_inducing, dataset, dtype)\n",
    "    def __init__(self, config: VariationalConfiguration, dataset, dtype=torch.float64):\n",
    "        self.config = config\n",
    "        super().__init__(config, dataset, dtype=dtype)\n",
    "        self.nfe = 0\n",
    "        self.f = None\n",
    "\n",
    "    def initial_state(self):\n",
    "        initial_state = torch.zeros(torch.Size([self.num_outputs, 1]), dtype=torch.float64)\n",
    "        initial_state = initial_state.cuda() if is_cuda() else initial_state\n",
    "        return initial_state.repeat(self.config.num_samples, 1, 1)  # Add batch dimension for sampling\n",
    "        # if self.options.initial_conditions: TODO:\n",
    "        #     h = self.initial_conditions.repeat(h.shape[0], 1, 1)\n",
    "\n",
    "    def forward(self, t, q_f, step_size=1e-1, return_samples=False):\n",
    "        \"\"\"\n",
    "        t : torch.Tensor\n",
    "            Shape (num_times)\n",
    "        h : torch.Tensor the initial state of the ODE\n",
    "            Shape (num_genes, 1)\n",
    "        Returns\n",
    "        -------\n",
    "        Returns evolved h across times t.\n",
    "        Shape (num_genes, num_points).\n",
    "        \"\"\"\n",
    "        self.nfe = 0\n",
    "\n",
    "        # Integrate forward from the initial positions h0.\n",
    "        h0 = self.initial_state()\n",
    "        print(h0.shape)\n",
    "        self.f = self.G(q_f.rsample([self.options.num_samples]))\n",
    "        print('de_model forward', self.f, self.f.shape)\n",
    "        # print(self.f.shape)\n",
    "        self.t_index = 0\n",
    "        self.last_t = self.f.min()-1\n",
    "        h_samples = odeint(self.odefunc, h0, t, method='rk4', options=dict(step_size=step_size))#, rtol=rtol, atol=atol)  # (T, S, num_outputs, 1)\n",
    "\n",
    "        self.f = None\n",
    "        # self.t_index = None\n",
    "        # self.last_t = None\n",
    "        if return_samples:\n",
    "            return h_samples\n",
    "\n",
    "        h_out = torch.mean(h_samples, dim=1).transpose(0, 1)\n",
    "        h_var = torch.var(h_samples, dim=1).transpose(0, 1)\n",
    "\n",
    "        return self.decode(h_out), h_var\n",
    "\n",
    "    def decode(self, h_out):\n",
    "        return h_out\n",
    "\n",
    "    @abstractmethod\n",
    "    def odefunc(self, t, h):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            h: shape (num_samples, num_outputs, 1)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def G(self, f):\n",
    "        return f.repeat(1, self.num_outputs, 1)  # (S, I, t)\n",
    "\n",
    "\n",
    "class TranscriptionalRegulationLFM(OrdinaryLFM):\n",
    "    def __init__(self, config: VariationalConfiguration, dataset):\n",
    "        super().__init__(config, dataset)\n",
    "        self.decay_rate = Parameter(0.1 + torch.rand((self.num_outputs, 1), dtype=torch.float64))\n",
    "        self.basal_rate = Parameter(torch.rand((self.num_outputs, 1), dtype=torch.float64))\n",
    "        self.sensitivity = Parameter(0.2 + torch.rand((self.num_outputs, 1), dtype=torch.float64))\n",
    "\n",
    "    def initial_state(self):\n",
    "        return (self.basal_rate / self.decay_rate).unsqueeze(0).repeat(self.batch_size, 1, 1)\n",
    "\n",
    "    def odefunc(self, t, h):\n",
    "        \"\"\"h is of shape (num_samples, num_outputs, 1)\"\"\"\n",
    "        self.nfe += 1\n",
    "        # if (self.nfe % 100) == 0:\n",
    "        #     print(t)\n",
    "\n",
    "        decay = self.decay_rate * h\n",
    "\n",
    "        f = self.f[:, :, self.t_index].unsqueeze(2)\n",
    "\n",
    "        h = self.basal_rate + self.sensitivity * f - decay\n",
    "        if t > self.last_t:\n",
    "            self.t_index += 1\n",
    "        self.last_t = t\n",
    "        return h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from lafomo.variational.trainer import Trainer\n",
    "from lafomo.utilities.torch import is_cuda\n",
    "\n",
    "\n",
    "class NewTrainer(Trainer):\n",
    "    def __init__(self, gp_model: gpytorch.models.GP, de_model, optimizer: torch.optim.Optimizer, dataset):\n",
    "        super().__init__(de_model, optimizer, dataset)\n",
    "        self.gp_model = gp_model\n",
    "\n",
    "    def single_epoch(self, rtol, atol):\n",
    "        data = next(iter(self.data_loader))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        t, y = data\n",
    "        t = t.cuda() if is_cuda() else t\n",
    "        y = y.cuda() if is_cuda() else y\n",
    "        # Assume that the batch of t s are the same\n",
    "        t, y = t[0].view(-1), y\n",
    "\n",
    "        # with ef.scan():\n",
    "        t_f = torch.arange(t.min(), t.max()+step_size/3, step_size/3)\n",
    "        print('feeding gp with ', t_f.shape)\n",
    "        f_output = self.gp_model(t_f)\n",
    "        print('fout', f_output)\n",
    "        print(type(f_output))\n",
    "\n",
    "        g_output = self.de_model(t, f_output, step_size=1e-1)\n",
    "\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(g_output, y)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "dataset = P53Data(replicate=0, data_dir='../data')\n",
    "num_genes = 5\n",
    "num_tfs = 1\n",
    "config = VariationalConfiguration(\n",
    "    preprocessing_variance=dataset.variance,\n",
    "    learn_inducing=False,\n",
    "    num_samples=70,\n",
    "    kernel_scale=False,\n",
    "    initial_conditions=False\n",
    ")\n",
    "\n",
    "# The shape of the inducing points should be (2 x m x 1) - so that we learn different inducing\n",
    "# points for each output\n",
    "num_inducing = 12\n",
    "inducing_points = torch.linspace(0, 12, num_inducing).view(1, num_inducing, 1)\n",
    "\n",
    "gp_model = MultitaskGPModel(inducing_points, num_tfs)\n",
    "de_model = TranscriptionalRegulationLFM(config, dataset)\n",
    "\n",
    "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=num_tfs)\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "gp_model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    *gp_model.parameters(),\n",
    "    *de_model.parameters()\n",
    "], lr=0.05)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "# num_data refers to the number of training datapoints\n",
    "num_training_points = 7 * 5\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, gp_model, num_training_points)\n",
    "trainer = NewTrainer(gp_model, de_model, optimizer, dataset)\n",
    "\n",
    "step_size = 1e-1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feeding gp with  torch.Size([362])\n",
      "fout MultitaskMultivariateNormal(loc: torch.Size([362]))\n",
      "<class 'gpytorch.distributions.multitask_multivariate_normal.MultitaskMultivariateNormal'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TranscriptionalRegulationLFM' object has no attribute 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-23-5211226416f3>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtrainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrtol\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstep_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\Documents\\proj\\lafomo\\lafomo\\variational\\trainer.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(self, epochs, report_interval, rtol, atol)\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     45\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 46\u001B[1;33m             \u001B[0mepoch_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msplit_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msingle_epoch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrtol\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0matol\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     47\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mepoch\u001B[0m \u001B[1;33m%\u001B[0m \u001B[0mreport_interval\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-20-d1fa58a1c871>\u001B[0m in \u001B[0;36msingle_epoch\u001B[1;34m(self, rtol, atol)\u001B[0m\n\u001B[0;32m     25\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 27\u001B[1;33m         \u001B[0mg_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mde_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mt\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf_output\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstep_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1e-1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     28\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m         \u001B[1;31m# Calc loss and backprop gradients\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\wishart\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    888\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 889\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-19-464b47c5b08b>\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, t, q_f, step_size, return_samples)\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     67\u001B[0m         \u001B[1;31m# Integrate forward from the initial positions h0.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 68\u001B[1;33m         \u001B[0mh0\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minitial_state\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     69\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mh0\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     70\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mG\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mq_f\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrsample\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnum_samples\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-19-464b47c5b08b>\u001B[0m in \u001B[0;36minitial_state\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    110\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    111\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0minitial_state\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 112\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbasal_rate\u001B[0m \u001B[1;33m/\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdecay_rate\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munsqueeze\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrepeat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    113\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    114\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0modefunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mh\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\wishart\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m    946\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mmodules\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    947\u001B[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001B[1;32m--> 948\u001B[1;33m             type(self).__name__, name))\n\u001B[0m\u001B[0;32m    949\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    950\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__setattr__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mUnion\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'Module'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'TranscriptionalRegulationLFM' object has no attribute 'batch_size'"
     ]
    }
   ],
   "source": [
    "trainer.train(1, rtol=step_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-222640c7",
   "language": "python",
   "display_name": "PyCharm (reggae)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}